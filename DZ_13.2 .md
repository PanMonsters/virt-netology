# Домашнее задание к занятию "13.2 разделы и монтирование"

---
## Задание 1: Подключить для тестового конфига общую папку

В stage окружении часто возникает необходимость отдавать статику бекенда сразу фронтом. Проще всего сделать это через общую папку. Требования:
* в поде подключена общая папка между контейнерами (например, /static);
* после записи чего-либо в контейнере с беком файлы можно получить из контейнера с фронтом.

## Решение 1: Подключить для тестового конфига общую папку

  - Устанавливаю NFS  
    - На мастере  
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash  
helm repo add stable https://charts.helm.sh/stable && helm repo update  
helm install nfs-server stable/nfs-server-provisioner  
apt install nfs-common -y  

    - На ноде  
apt install nfs-common -y  

<details><summary>deployment.yml</summary>

```
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: apps
  name: apps
  namespace: default
spec:
  selector:
    matchLabels:
      app: apps
  replicas: 1
  template:
    metadata:
      labels:
        app: apps
    spec:
      containers:
      - image: panmonster/netorep:my_frontend
        imagePullPolicy: IfNotPresent
        name: backend
        volumeMounts:
          - mountPath: "/static"
            name: static
      - image: panmonster/netorep:my_backend
        imagePullPolicy: IfNotPresent
        name: frontend
        volumeMounts:
          - mountPath: "/tmp/cache"
            name: static
      volumes:
        - name: static
          emptyDir: {}
```

</details>

<details><summary>Результат</summary>

```

vagrant@node1:~$ kubectl get pods
NAME                                  READY   STATUS      RESTARTS       AGE
apps-75596b9b96-lftj8                 2/2     Running     1 (32m ago)   174m
nfs-server-nfs-server-provisioner-0   1/1     Running     0             161m

vagrant@node1:~$ kubectl exec apps-75596b9b96-lftj8 -c backend -- sh -c "echo 'test' > /static/test.txt"

vagrant@node1:~$ kubectl exec apps-75596b9b96-lftj8 -c frontend -- ls -l /tmp/cache/
total 4
-rw-r--r-- 1 root root 5 Sep 20 13:34 test.txt

vagrant@node1:~$ kubectl exec apps-75596b9b96-lftj8 -c frontend -- cat /tmp/cache/test.txt
test
```

</details>

## Задание 2: Подключить общую папку для прода

Поработав на stage, доработки нужно отправить на прод. В продуктиве у нас контейнеры крутятся в разных подах, поэтому потребуется PV и связь через PVC. Сам PV должен быть связан с NFS сервером. Требования:
* все бекенды подключаются к одному PV в режиме ReadWriteMany;
* фронтенды тоже подключаются к этому же PV с таким же режимом;
* файлы, созданные бекендом, должны быть доступны фронту.



## Решение 2: Подключить общую папку для прода

<details><summary>pvc.yaml</summary>

```
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc
spec:
  storageClassName: "test"
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

</details>

<details><summary>pv.yaml</summary>

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv
spec:
  storageClassName: "test"
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 1Gi
  hostPath:
    path: /data/pv
```

</details>

<details><summary>backend.yml</summary>

```
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: backend
  name: backend
  namespace: default
spec:
  selector:
    matchLabels:
      app: backend
  replicas: 1
  template:
    metadata:
      labels:
        app: backend
    spec:
      initContainers:
      containers:
      - image: panmonster/netorep:my_backend
        imagePullPolicy: IfNotPresent
        name: backend
        env:
          - name: DATABASE_URL
            value: postgres://postgres:postgres@db:5432/news
        volumeMounts:
          - mountPath: "/static"
            name: my-volume
      volumes:
        - name: my-volume
          persistentVolumeClaim:
            claimName: pvc
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: backend
  name: backend
  namespace: default
spec:
  type: ClusterIP
  ports:
  - name: backend
    port: 9000
    protocol: TCP
    targetPort: 9000
  selector:
    app: backend
```

</details>

<details><summary>frontend.yml</summary>

```
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: frontend
  name: frontend
  namespace: default
spec:
  selector:
    matchLabels:
      app: frontend
  replicas: 1
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
      - image: panmonster/netorep:my_frontend
        imagePullPolicy: IfNotPresent
        name: frontend
        env:
          - name: BASE_URL
            value: http://backend:9000
        volumeMounts:
          - mountPath: "/tmp/cache"
            name: my-volume
      volumes:
        - name: my-volume
          persistentVolumeClaim:
            claimName: pvc
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: frontend
  name: frontend
  namespace: default
spec:
  type: ClusterIP
  ports:
  - name: frontend
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: frontend
```

</details>

<details><summary>Результат</summary>

 - Проеряю работу волюма
 
```
vagrant@node1:~$ kubectl get po,pv,pvc
NAME                                      READY   STATUS    RESTARTS      AGE
pod/backend-b88b5768c-ww48v               1/1     Running   0             19m
pod/frontend-649b45f857-hgm9t             1/1     Running   0             15m
pod/nfs-server-nfs-server-provisioner-0   1/1     Running   1 (75m ago)   3h27m

NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE
persistentvolume/pv   1Gi        RWO            Retain           Bound    default/pvc                           36m

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/pvc   Bound    pv       2Gi        RWO                           36m

vagrant@node1:~$ kubectl exec backend-b88b5768c-ww48v -c backend -- sh -c "echo 'test' > /static/test.txt"

vagrant@node1:~$ kubectl exec frontend-649b45f857-hgm9t -c frontend -- ls -l /tmp/cache/
total 4
-rw-r--r-- 1 root root 5 Sep 20 15:47 test.txt

vagrant@node1:~$ kubectl exec frontend-649b45f857-hgm9t -c frontend -- cat /tmp/cache/test.txt
test
```

 - Пересоздаю поды и проверяю сохранность информации на волюме
 
 ```
vagrant@node1:~$ kubectl delete deployment backend
deployment.apps "backend" deleted

vagrant@node1:~$ kubectl delete deployment frontend
deployment.apps "frontend" deleted

vagrant@node1:~$ kubectl get po,pv,pvc
NAME                                      READY   STATUS    RESTARTS      AGE
pod/nfs-server-nfs-server-provisioner-0   1/1     Running   1 (80m ago)   3h32m

NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE
persistentvolume/pv   1Gi        RWO            Retain           Bound    default/pvc                           40m

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/pvc   Bound    pv       2Gi        RWO                           41m

vagrant@node1:~$ kubectl apply -f bak.yml
deployment.apps/backend created
service/backend unchanged

vagrant@node1:~$ kubectl apply -f front.yml
deployment.apps/frontend created
service/frontend unchanged

vagrant@node1:~$ kubectl get po,pv,pvc
NAME                                      READY   STATUS    RESTARTS      AGE
pod/backend-b88b5768c-znwmh               1/1     Running   0             13s
pod/frontend-649b45f857-nl8wr             1/1     Running   0             6s
pod/nfs-server-nfs-server-provisioner-0   1/1     Running   1 (80m ago)   3h32m

NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM         STORAGECLASS   REASON   AGE
persistentvolume/pv   1Gi        RWO            Retain           Bound    default/pvc                           41m

NAME                        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/pvc   Bound    pv       2Gi        RWO                           42m

vagrant@node1:~$ kubectl exec frontend-649b45f857-nl8wr -c frontend -- cat /tmp/cache/test.txt
test
 ```
 
 - Проверяю наличие созданного файла локально
 
```
vagrant@node2:~$ cd /data/pv

vagrant@node2:/data/pv$ ls
test.txt

vagrant@node2:/data/pv$
```

</details>
