# Домашнее задание к занятию "12.5 Сетевые решения CNI"

---
## Задание 1: установить в кластер CNI плагин Calico

Для проверки других сетевых решений стоит поставить отличный от Flannel плагин — например, Calico. Требования: 
* установка производится через ansible/kubespray;
* после применения следует настроить политику доступа к hello-world извне. Инструкции [kubernetes.io](https://kubernetes.io/docs/concepts/services-networking/network-policies/), [Calico](https://docs.projectcalico.org/about/about-network-policy)


## Решение 1: установить в кластер CNI плагин Calico

 - Поднимаю кластер и разворачиваю 2 пода hello-node-6d5f754cc9-76db7 и hello-node-6d5f754cc9-v4g7k в деплойменте hello-node

```
ubuntu@fhml2qd2qhgnln7s9cjp:~/kubespray$ mkdir -p $HOME/.kube
ubuntu@fhml2qd2qhgnln7s9cjp:~/kubespray$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
ubuntu@fhml2qd2qhgnln7s9cjp:~/kubespray$ sudo chown $(id -u):$(id -g) $HOME/.kube/config

ubuntu@fhml2qd2qhgnln7s9cjp:~/kubespray$ kubectl get nodes -o wide
NAME       STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
control1   Ready    control-plane   13m   v1.24.4   10.128.0.26   <none>        Ubuntu 20.04.5 LTS   5.4.0-124-generic   containerd://1.6.8
node2      Ready    <none>          12m   v1.24.4   10.128.0.8    <none>        Ubuntu 20.04.5 LTS   5.4.0-124-generic   containerd://1.6.8

ubuntu@fhml2qd2qhgnln7s9cjp:~/kubespray$ kubectl create deployment hello-node --image=k8s.gcr.io/echoserver:1.4 --replicas=2

ubuntu@fhml2qd2qhgnln7s9cjp:~/kubespray$ kubectl get deploy -o wide
NAME         READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                      SELECTOR
hello-node   2/2     2            2           6s    echoserver   k8s.gcr.io/echoserver:1.4   app=hello-node

ubuntu@fhml2qd2qhgnln7s9cjp:~/kubespray$ kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES
hello-node-6d5f754cc9-76db7   1/1     Running   0          62s   10.233.75.5   node2   <none>           <none>
hello-node-6d5f754cc9-v4g7k   1/1     Running   0          62s   10.233.75.4   node2   <none>           <none>
```

 - Создаю службу на основе полученых подов для доступа из вне
 
```
ubuntu@control1:~$ kubectl expose deployment hello-node --type=LoadBalancer --port=8080
service/hello-node exposed

ubuntu@control1:~$ kubectl get services -o wide
NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE   SELECTOR
hello-node   LoadBalancer   10.233.10.144   <pending>     8080:31912/TCP   13s   app=hello-node
kubernetes   ClusterIP      10.233.0.1      <none>        443/TCP          33m   <none>
```

 - Проверяю наличие политик, а также доступ с пода на под и доступ из вне

```
ubuntu@control1:~$ kubectl get netpol -A
No resources found

Доступ с ноды на ноду
ubuntu@control1:~$ kubectl exec hello-node-6d5f754cc9-76db7 -- curl -m 1 -s http://10.233.75.4:8080 | grep -e request_uri -e host -e client_address
client_address=10.233.75.5
request_uri=http://10.233.75.4:8080/
host=10.233.75.4:8080

ubuntu@control1:~$ kubectl exec hello-node-6d5f754cc9-v4g7k -- curl -m 1 -s http://10.233.75.5:8080 | grep -e request_uri -e host -e client_address
client_address=10.233.75.4
request_uri=http://10.233.75.5:8080/
host=10.233.75.5:8080

panmonster@panmonster-PC:~/terraform_for_kuber$ curl -s -m 1 http://178.154.206.122:31912 | grep -e request_uri -e host -e client_address
client_address=10.128.0.8
request_uri=http://178.154.206.122:8080/
host=178.154.206.122:31912
```

 - Создаю необходимые политики

<details><summary>default-ingress-deny.yml</summary>

```
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector:
    matchLabels:
      app: hello-node
  policyTypes:
  - Ingress
  - Egress
```

</details>

<details><summary>pods-on-pod-ingress.yml</summary>

```
---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: pods-on-pod-ingress
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: hello-node
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: hello-node
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: hello-node
```		  
		  
</details>

 - Применяю вышеупомянутые политики, а также проверяю доступ с пода на под и доступ из вне

```
default-ingress-deny.yml  hosts.yaml  pods-on-pod-ingress.yml
ubuntu@control1:~/kubespray/inventory/mycluster$ kubectl apply -f default-ingress-deny.yml
networkpolicy.networking.k8s.io/default-deny-ingress created
ubuntu@control1:~/kubespray/inventory/mycluster$ kubectl apply -f pods-on-pod-ingress.yml
networkpolicy.networking.k8s.io/pods-on-pod-ingress created

ubuntu@control1:~$ kubectl get netpol -A
NAMESPACE   NAME                   POD-SELECTOR     AGE
default     default-deny-ingress   app=hello-node   5m37s
default     pods-on-pod-ingress    app=hello-node   5m22s

ubuntu@control1:~/kubespray/inventory/mycluster$ kubectl exec hello-node-6d5f754cc9-76db7 -- curl -m 1 -s http://10.233.75.4:8080 | grep -e request_uri -e host -e client_address
client_address=10.233.75.5
request_uri=http://10.233.75.4:8080/
host=10.233.75.4:8080
ubuntu@control1:~/kubespray/inventory/mycluster$ kubectl exec hello-node-6d5f754cc9-v4g7k -- curl -m 1 -s http://10.233.75.5:8080 | grep -e request_uri -e host -e client_address
client_address=10.233.75.4
request_uri=http://10.233.75.5:8080/
host=10.233.75.5:8080

panmonster@panmonster-PC:~/terraform_for_kuber$ curl -s -v -m 1 http://178.154.206.122:31912
*   Trying 178.154.206.122:31912...
* Connection timed out after 1001 milliseconds
* Closing connection 0
```

## Задание 2: изучить, что запущено по умолчанию

Самый простой способ — проверить командой calicoctl get <type>. Для проверки стоит получить список нод, ipPool и profile.
Требования: 
* установить утилиту calicoctl;
* получить 3 вышеописанных типа в консоли.

## Решение 2: изучить, что запущено по умолчанию

```
ubuntu@control1:~/kubespray/inventory/mycluster$ curl -L https://github.com/projectcalico/calico/releases/download/v3.23.3/calicoctl-linux-amd64 -o calicoctl
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 56.5M  100 56.5M    0     0  17.9M      0  0:00:03  0:00:03 --:--:-- 25.3M
ubuntu@control1:~/kubespray/inventory/mycluster$ chmod +x ./calicoctl


ubuntu@control1:~/kubespray/inventory/mycluster$ ./calicoctl get node
NAME
control1
node2


ubuntu@control1:~/kubespray/inventory/mycluster$ ./calicoctl get ipPool
NAME           CIDR             SELECTOR
default-pool   10.233.64.0/18   all()

ubuntu@control1:~/kubespray/inventory/mycluster$ ./calicoctl get profile
NAME
projectcalico-default-allow
kns.default
kns.kube-node-lease
kns.kube-public
kns.kube-system
ksa.default.default
ksa.kube-node-lease.default
ksa.kube-public.default
ksa.kube-system.attachdetach-controller
ksa.kube-system.bootstrap-signer
ksa.kube-system.calico-node
ksa.kube-system.certificate-controller
ksa.kube-system.clusterrole-aggregation-controller
ksa.kube-system.coredns
ksa.kube-system.cronjob-controller
ksa.kube-system.daemon-set-controller
ksa.kube-system.default
ksa.kube-system.deployment-controller
ksa.kube-system.disruption-controller
ksa.kube-system.dns-autoscaler
ksa.kube-system.endpoint-controller
ksa.kube-system.endpointslice-controller
ksa.kube-system.endpointslicemirroring-controller
ksa.kube-system.ephemeral-volume-controller
ksa.kube-system.expand-controller
ksa.kube-system.generic-garbage-collector
ksa.kube-system.horizontal-pod-autoscaler
ksa.kube-system.job-controller
ksa.kube-system.kube-proxy
ksa.kube-system.namespace-controller
ksa.kube-system.node-controller
ksa.kube-system.nodelocaldns
ksa.kube-system.persistent-volume-binder
ksa.kube-system.pod-garbage-collector
ksa.kube-system.pv-protection-controller
ksa.kube-system.pvc-protection-controller
ksa.kube-system.replicaset-controller
ksa.kube-system.replication-controller
ksa.kube-system.resourcequota-controller
ksa.kube-system.root-ca-cert-publisher
ksa.kube-system.service-account-controller
ksa.kube-system.service-controller
ksa.kube-system.statefulset-controller
ksa.kube-system.token-cleaner
ksa.kube-system.ttl-after-finished-controller
ksa.kube-system.ttl-controller
```